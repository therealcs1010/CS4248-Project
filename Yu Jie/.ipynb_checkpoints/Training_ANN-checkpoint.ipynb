{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36a766c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "\n",
    "spacy.require_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95749cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2492f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_directory = '../processed_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3f6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('../processed_data/train.json')\n",
    "test_df = pd.read_json('../processed_data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d586b5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3530</td>\n",
       "      <td>I can safely admit (as an IMDb geek) that 'Pha...</td>\n",
       "      <td>7</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4850</td>\n",
       "      <td>So your bairns are away on a sleep-over ? The ...</td>\n",
       "      <td>10</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4027</td>\n",
       "      <td>This must be one of the most overrated Spanish...</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>971</td>\n",
       "      <td>This film is a bit reminiscent of the German f...</td>\n",
       "      <td>8</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11140</td>\n",
       "      <td>John Schlesinger's 'Midnight Cowboy' is perhap...</td>\n",
       "      <td>9</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>7383</td>\n",
       "      <td>I feel blessed to own what is known as the wor...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>7423</td>\n",
       "      <td>I recently watched the '54 version of this fil...</td>\n",
       "      <td>9</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>5991</td>\n",
       "      <td>A sequel to Angels With Dirty Faces in name on...</td>\n",
       "      <td>8</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>3118</td>\n",
       "      <td>Having not seen the previous two in the trilog...</td>\n",
       "      <td>9</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>3771</td>\n",
       "      <td>This has to be one of the worst films of the 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  rating label\n",
       "0       3530  I can safely admit (as an IMDb geek) that 'Pha...       7     +\n",
       "1       4850  So your bairns are away on a sleep-over ? The ...      10     +\n",
       "2       4027  This must be one of the most overrated Spanish...       3     -\n",
       "3        971  This film is a bit reminiscent of the German f...       8     +\n",
       "4      11140  John Schlesinger's 'Midnight Cowboy' is perhap...       9     +\n",
       "...      ...                                                ...     ...   ...\n",
       "24995   7383  I feel blessed to own what is known as the wor...       1     -\n",
       "24996   7423  I recently watched the '54 version of this fil...       9     +\n",
       "24997   5991  A sequel to Angels With Dirty Faces in name on...       8     +\n",
       "24998   3118  Having not seen the previous two in the trilog...       9     +\n",
       "24999   3771  This has to be one of the worst films of the 1...       1     -\n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edcbbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>10</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000</td>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>7</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>9</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002</td>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>8</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003</td>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>8</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>9998</td>\n",
       "      <td>I occasionally let my kids watch this garbage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>9999</td>\n",
       "      <td>When all we have anymore is pretty much realit...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>999</td>\n",
       "      <td>The basic genre is a thriller intercut with an...</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>99</td>\n",
       "      <td>Four things intrigued me as to this film - fir...</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>9</td>\n",
       "      <td>David Bryce's comments nearby are exceptionall...</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  rating label\n",
       "0          0  I went and saw this movie last night after bei...      10     +\n",
       "1      10000  Actor turned director Bill Paxton follows up h...       7     +\n",
       "2      10001  As a recreational golfer with some knowledge o...       9     +\n",
       "3      10002  I saw this film in a sneak preview, and it is ...       8     +\n",
       "4      10003  Bill Paxton has taken the true story of the 19...       8     +\n",
       "...      ...                                                ...     ...   ...\n",
       "24995   9998  I occasionally let my kids watch this garbage ...       1     -\n",
       "24996   9999  When all we have anymore is pretty much realit...       1     -\n",
       "24997    999  The basic genre is a thriller intercut with an...       3     -\n",
       "24998     99  Four things intrigued me as to this film - fir...       3     -\n",
       "24999      9  David Bryce's comments nearby are exceptionall...       4     -\n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6eff201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x2566d77eee0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x2566d77e640>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x2566d74a7b0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x25673c97e00>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x25673c8b5c0>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x2566d74a660>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e76b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    return doc.vector\n",
    "\n",
    "def get_features(df):\n",
    "    text_list = df['text'].to_list()\n",
    "    x = []\n",
    "    for doc in tqdm_notebook(nlp.pipe(text_list, disable=[\"tagger\", \"parser\", \"lemmatizer\", 'attribute_ruler', 'ner']),total=len(text_list) ):\n",
    "        x.append(doc.vector.get())\n",
    "    x = pd.DataFrame(x)\n",
    "    y = df['label']\n",
    "    y = pd.get_dummies(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b73ed6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d6bc8a2cd8446aa92b19b10933bfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4127e988c0405aa1c2f7931ab20c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x, train_y = get_features(train_df)\n",
    "test_x, test_y = get_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9e92a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006977</td>\n",
       "      <td>0.152942</td>\n",
       "      <td>-0.131309</td>\n",
       "      <td>-0.036154</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.012373</td>\n",
       "      <td>-0.016544</td>\n",
       "      <td>-0.126087</td>\n",
       "      <td>-0.029805</td>\n",
       "      <td>2.124057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164012</td>\n",
       "      <td>0.049071</td>\n",
       "      <td>0.032471</td>\n",
       "      <td>-0.056707</td>\n",
       "      <td>0.007302</td>\n",
       "      <td>-0.010890</td>\n",
       "      <td>-0.064989</td>\n",
       "      <td>-0.071053</td>\n",
       "      <td>-0.014184</td>\n",
       "      <td>0.045542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.130510</td>\n",
       "      <td>-0.104212</td>\n",
       "      <td>-0.063575</td>\n",
       "      <td>0.084911</td>\n",
       "      <td>-0.025344</td>\n",
       "      <td>-0.035447</td>\n",
       "      <td>-0.114975</td>\n",
       "      <td>-0.006386</td>\n",
       "      <td>1.935403</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190982</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>-0.029648</td>\n",
       "      <td>0.027210</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>-0.044958</td>\n",
       "      <td>-0.012830</td>\n",
       "      <td>0.056736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.097324</td>\n",
       "      <td>0.184632</td>\n",
       "      <td>-0.128490</td>\n",
       "      <td>-0.084664</td>\n",
       "      <td>0.054203</td>\n",
       "      <td>0.032274</td>\n",
       "      <td>0.047223</td>\n",
       "      <td>-0.114767</td>\n",
       "      <td>0.014312</td>\n",
       "      <td>2.152641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172063</td>\n",
       "      <td>0.022724</td>\n",
       "      <td>-0.101178</td>\n",
       "      <td>-0.023656</td>\n",
       "      <td>0.044661</td>\n",
       "      <td>0.068299</td>\n",
       "      <td>-0.024370</td>\n",
       "      <td>-0.091496</td>\n",
       "      <td>-0.002683</td>\n",
       "      <td>0.093178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.027772</td>\n",
       "      <td>0.130567</td>\n",
       "      <td>-0.152985</td>\n",
       "      <td>-0.047217</td>\n",
       "      <td>0.071326</td>\n",
       "      <td>0.016790</td>\n",
       "      <td>-0.003706</td>\n",
       "      <td>-0.101126</td>\n",
       "      <td>-0.009193</td>\n",
       "      <td>2.152511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233026</td>\n",
       "      <td>0.042364</td>\n",
       "      <td>-0.024650</td>\n",
       "      <td>-0.034881</td>\n",
       "      <td>0.024407</td>\n",
       "      <td>0.030208</td>\n",
       "      <td>-0.044393</td>\n",
       "      <td>-0.083346</td>\n",
       "      <td>0.033778</td>\n",
       "      <td>0.086929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.027660</td>\n",
       "      <td>0.171387</td>\n",
       "      <td>-0.097587</td>\n",
       "      <td>-0.055611</td>\n",
       "      <td>0.107496</td>\n",
       "      <td>0.039108</td>\n",
       "      <td>0.027723</td>\n",
       "      <td>-0.104099</td>\n",
       "      <td>-0.018723</td>\n",
       "      <td>2.075550</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155245</td>\n",
       "      <td>0.034445</td>\n",
       "      <td>-0.011114</td>\n",
       "      <td>-0.040261</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.027404</td>\n",
       "      <td>-0.046025</td>\n",
       "      <td>-0.048126</td>\n",
       "      <td>-0.025229</td>\n",
       "      <td>0.035457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>-0.030652</td>\n",
       "      <td>0.130034</td>\n",
       "      <td>-0.115811</td>\n",
       "      <td>-0.049625</td>\n",
       "      <td>0.089282</td>\n",
       "      <td>-0.006514</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>-0.112348</td>\n",
       "      <td>-0.015421</td>\n",
       "      <td>2.150479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164964</td>\n",
       "      <td>0.049693</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>-0.068340</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.045247</td>\n",
       "      <td>-0.028728</td>\n",
       "      <td>0.007879</td>\n",
       "      <td>0.023769</td>\n",
       "      <td>0.092720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>-0.030462</td>\n",
       "      <td>0.185232</td>\n",
       "      <td>-0.091917</td>\n",
       "      <td>-0.073782</td>\n",
       "      <td>0.093834</td>\n",
       "      <td>0.055192</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>-0.165647</td>\n",
       "      <td>-0.020743</td>\n",
       "      <td>2.113348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188884</td>\n",
       "      <td>0.054723</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>-0.019912</td>\n",
       "      <td>0.050026</td>\n",
       "      <td>-0.051562</td>\n",
       "      <td>-0.068628</td>\n",
       "      <td>-0.098007</td>\n",
       "      <td>-0.023132</td>\n",
       "      <td>0.143531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>-0.006854</td>\n",
       "      <td>0.121101</td>\n",
       "      <td>-0.092478</td>\n",
       "      <td>-0.042141</td>\n",
       "      <td>0.070015</td>\n",
       "      <td>-0.053947</td>\n",
       "      <td>0.029291</td>\n",
       "      <td>-0.094470</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>1.957572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149024</td>\n",
       "      <td>0.020803</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>-0.023384</td>\n",
       "      <td>-0.011275</td>\n",
       "      <td>0.019338</td>\n",
       "      <td>-0.048050</td>\n",
       "      <td>-0.083139</td>\n",
       "      <td>-0.070448</td>\n",
       "      <td>0.045597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0.011233</td>\n",
       "      <td>0.128624</td>\n",
       "      <td>-0.120690</td>\n",
       "      <td>-0.084393</td>\n",
       "      <td>0.079866</td>\n",
       "      <td>0.054415</td>\n",
       "      <td>0.022280</td>\n",
       "      <td>-0.146740</td>\n",
       "      <td>0.024745</td>\n",
       "      <td>2.133624</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178119</td>\n",
       "      <td>0.093841</td>\n",
       "      <td>-0.012928</td>\n",
       "      <td>-0.043249</td>\n",
       "      <td>0.075504</td>\n",
       "      <td>-0.001530</td>\n",
       "      <td>-0.012723</td>\n",
       "      <td>-0.033844</td>\n",
       "      <td>-0.008538</td>\n",
       "      <td>0.097597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>-0.033906</td>\n",
       "      <td>0.118301</td>\n",
       "      <td>-0.111804</td>\n",
       "      <td>-0.031310</td>\n",
       "      <td>0.080290</td>\n",
       "      <td>0.008416</td>\n",
       "      <td>-0.028345</td>\n",
       "      <td>-0.160925</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>2.289585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201878</td>\n",
       "      <td>0.036660</td>\n",
       "      <td>0.023464</td>\n",
       "      <td>-0.060458</td>\n",
       "      <td>0.018191</td>\n",
       "      <td>0.053610</td>\n",
       "      <td>-0.047162</td>\n",
       "      <td>-0.025714</td>\n",
       "      <td>-0.016015</td>\n",
       "      <td>0.119650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.006977  0.152942 -0.131309 -0.036154  0.122642  0.012373 -0.016544   \n",
       "1      0.012072  0.130510 -0.104212 -0.063575  0.084911 -0.025344 -0.035447   \n",
       "2     -0.097324  0.184632 -0.128490 -0.084664  0.054203  0.032274  0.047223   \n",
       "3     -0.027772  0.130567 -0.152985 -0.047217  0.071326  0.016790 -0.003706   \n",
       "4     -0.027660  0.171387 -0.097587 -0.055611  0.107496  0.039108  0.027723   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24995 -0.030652  0.130034 -0.115811 -0.049625  0.089282 -0.006514  0.038002   \n",
       "24996 -0.030462  0.185232 -0.091917 -0.073782  0.093834  0.055192  0.038778   \n",
       "24997 -0.006854  0.121101 -0.092478 -0.042141  0.070015 -0.053947  0.029291   \n",
       "24998  0.011233  0.128624 -0.120690 -0.084393  0.079866  0.054415  0.022280   \n",
       "24999 -0.033906  0.118301 -0.111804 -0.031310  0.080290  0.008416 -0.028345   \n",
       "\n",
       "            7         8         9    ...       290       291       292  \\\n",
       "0     -0.126087 -0.029805  2.124057  ... -0.164012  0.049071  0.032471   \n",
       "1     -0.114975 -0.006386  1.935403  ... -0.190982  0.006450  0.008672   \n",
       "2     -0.114767  0.014312  2.152641  ... -0.172063  0.022724 -0.101178   \n",
       "3     -0.101126 -0.009193  2.152511  ... -0.233026  0.042364 -0.024650   \n",
       "4     -0.104099 -0.018723  2.075550  ... -0.155245  0.034445 -0.011114   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24995 -0.112348 -0.015421  2.150479  ... -0.164964  0.049693  0.025036   \n",
       "24996 -0.165647 -0.020743  2.113348  ... -0.188884  0.054723  0.005237   \n",
       "24997 -0.094470  0.001424  1.957572  ... -0.149024  0.020803  0.017507   \n",
       "24998 -0.146740  0.024745  2.133624  ... -0.178119  0.093841 -0.012928   \n",
       "24999 -0.160925  0.004423  2.289585  ... -0.201878  0.036660  0.023464   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "0     -0.056707  0.007302 -0.010890 -0.064989 -0.071053 -0.014184  0.045542  \n",
       "1     -0.029648  0.027210  0.010179  0.024023 -0.044958 -0.012830  0.056736  \n",
       "2     -0.023656  0.044661  0.068299 -0.024370 -0.091496 -0.002683  0.093178  \n",
       "3     -0.034881  0.024407  0.030208 -0.044393 -0.083346  0.033778  0.086929  \n",
       "4     -0.040261  0.006755  0.027404 -0.046025 -0.048126 -0.025229  0.035457  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "24995 -0.068340  0.005200  0.045247 -0.028728  0.007879  0.023769  0.092720  \n",
       "24996 -0.019912  0.050026 -0.051562 -0.068628 -0.098007 -0.023132  0.143531  \n",
       "24997 -0.023384 -0.011275  0.019338 -0.048050 -0.083139 -0.070448  0.045597  \n",
       "24998 -0.043249  0.075504 -0.001530 -0.012723 -0.033844 -0.008538  0.097597  \n",
       "24999 -0.060458  0.018191  0.053610 -0.047162 -0.025714 -0.016015  0.119650  \n",
       "\n",
       "[25000 rows x 300 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d0f64ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>+</th>\n",
       "      <th>-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       +  -\n",
       "0      1  0\n",
       "1      1  0\n",
       "2      0  1\n",
       "3      1  0\n",
       "4      1  0\n",
       "...   .. ..\n",
       "24995  0  1\n",
       "24996  1  0\n",
       "24997  1  0\n",
       "24998  1  0\n",
       "24999  0  1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5bfeec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, input_shape=(300,), activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "callbacks=[\n",
    "    ModelCheckpoint(filepath='./models/checkpoint',\n",
    "                   save_best_only=True, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4143131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.6933\n",
      "Epoch 1: val_loss improved from inf to 0.69240, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.6933 - val_loss: 0.6924\n",
      "Epoch 2/200\n",
      "535/547 [============================>.] - ETA: 0s - loss: 0.6926\n",
      "Epoch 2: val_loss improved from 0.69240 to 0.69144, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.6926 - val_loss: 0.6914\n",
      "Epoch 3/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.6922\n",
      "Epoch 3: val_loss improved from 0.69144 to 0.68983, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.6922 - val_loss: 0.6898\n",
      "Epoch 4/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.6912\n",
      "Epoch 4: val_loss improved from 0.68983 to 0.68695, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.6911 - val_loss: 0.6869\n",
      "Epoch 5/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.6887\n",
      "Epoch 5: val_loss improved from 0.68695 to 0.68127, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.6887 - val_loss: 0.6813\n",
      "Epoch 6/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.6843\n",
      "Epoch 6: val_loss improved from 0.68127 to 0.66918, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.6843 - val_loss: 0.6692\n",
      "Epoch 7/200\n",
      "533/547 [============================>.] - ETA: 0s - loss: 0.6705\n",
      "Epoch 7: val_loss improved from 0.66918 to 0.63248, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.6701 - val_loss: 0.6325\n",
      "Epoch 8/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.6186\n",
      "Epoch 8: val_loss improved from 0.63248 to 0.52249, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.6181 - val_loss: 0.5225\n",
      "Epoch 9/200\n",
      "534/547 [============================>.] - ETA: 0s - loss: 0.5315\n",
      "Epoch 9: val_loss improved from 0.52249 to 0.51418, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.5298 - val_loss: 0.5142\n",
      "Epoch 10/200\n",
      "533/547 [============================>.] - ETA: 0s - loss: 0.4950\n",
      "Epoch 10: val_loss improved from 0.51418 to 0.41207, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.4957 - val_loss: 0.4121\n",
      "Epoch 11/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.4765\n",
      "Epoch 11: val_loss improved from 0.41207 to 0.40016, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.4772 - val_loss: 0.4002\n",
      "Epoch 12/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.4592\n",
      "Epoch 12: val_loss did not improve from 0.40016\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.4592 - val_loss: 0.4206\n",
      "Epoch 13/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.4474\n",
      "Epoch 13: val_loss improved from 0.40016 to 0.38322, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.4461 - val_loss: 0.3832\n",
      "Epoch 14/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.4316\n",
      "Epoch 14: val_loss improved from 0.38322 to 0.37559, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.4310 - val_loss: 0.3756\n",
      "Epoch 15/200\n",
      "533/547 [============================>.] - ETA: 0s - loss: 0.4281\n",
      "Epoch 15: val_loss did not improve from 0.37559\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.4276 - val_loss: 0.4944\n",
      "Epoch 16/200\n",
      "533/547 [============================>.] - ETA: 0s - loss: 0.4258\n",
      "Epoch 16: val_loss improved from 0.37559 to 0.37545, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.4248 - val_loss: 0.3754\n",
      "Epoch 17/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.4125\n",
      "Epoch 17: val_loss did not improve from 0.37545\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.4120 - val_loss: 0.6129\n",
      "Epoch 18/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.4136\n",
      "Epoch 18: val_loss improved from 0.37545 to 0.36870, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.4139 - val_loss: 0.3687\n",
      "Epoch 19/200\n",
      "542/547 [============================>.] - ETA: 0s - loss: 0.4008\n",
      "Epoch 19: val_loss improved from 0.36870 to 0.35752, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.4005 - val_loss: 0.3575\n",
      "Epoch 20/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.4034\n",
      "Epoch 20: val_loss did not improve from 0.35752\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.4021 - val_loss: 0.3819\n",
      "Epoch 21/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3966\n",
      "Epoch 21: val_loss did not improve from 0.35752\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3966 - val_loss: 0.3587\n",
      "Epoch 22/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3957\n",
      "Epoch 22: val_loss did not improve from 0.35752\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3956 - val_loss: 0.4447\n",
      "Epoch 23/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.3933\n",
      "Epoch 23: val_loss did not improve from 0.35752\n",
      "547/547 [==============================] - 2s 3ms/step - loss: 0.3931 - val_loss: 0.3636\n",
      "Epoch 24/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3884\n",
      "Epoch 24: val_loss improved from 0.35752 to 0.34722, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.3894 - val_loss: 0.3472\n",
      "Epoch 25/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3846\n",
      "Epoch 25: val_loss did not improve from 0.34722\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3846 - val_loss: 0.3598\n",
      "Epoch 26/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3856\n",
      "Epoch 26: val_loss improved from 0.34722 to 0.34682, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.3859 - val_loss: 0.3468\n",
      "Epoch 27/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.3816\n",
      "Epoch 27: val_loss did not improve from 0.34682\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3818 - val_loss: 0.3890\n",
      "Epoch 28/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3819\n",
      "Epoch 28: val_loss did not improve from 0.34682\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3825 - val_loss: 0.3781\n",
      "Epoch 29/200\n",
      "530/547 [============================>.] - ETA: 0s - loss: 0.3776\n",
      "Epoch 29: val_loss did not improve from 0.34682\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3787 - val_loss: 0.3567\n",
      "Epoch 30/200\n",
      "533/547 [============================>.] - ETA: 0s - loss: 0.3769\n",
      "Epoch 30: val_loss did not improve from 0.34682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3791 - val_loss: 0.3610\n",
      "Epoch 31/200\n",
      "531/547 [============================>.] - ETA: 0s - loss: 0.3738\n",
      "Epoch 31: val_loss did not improve from 0.34682\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3736 - val_loss: 0.3849\n",
      "Epoch 32/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3699\n",
      "Epoch 32: val_loss improved from 0.34682 to 0.33821, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.3704 - val_loss: 0.3382\n",
      "Epoch 33/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3704\n",
      "Epoch 33: val_loss did not improve from 0.33821\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3706 - val_loss: 0.3589\n",
      "Epoch 34/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3654\n",
      "Epoch 34: val_loss did not improve from 0.33821\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3650 - val_loss: 0.3898\n",
      "Epoch 35/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.3708\n",
      "Epoch 35: val_loss did not improve from 0.33821\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3708 - val_loss: 0.3519\n",
      "Epoch 36/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3687\n",
      "Epoch 36: val_loss improved from 0.33821 to 0.33432, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.3684 - val_loss: 0.3343\n",
      "Epoch 37/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3671\n",
      "Epoch 37: val_loss did not improve from 0.33432\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3656 - val_loss: 0.3426\n",
      "Epoch 38/200\n",
      "529/547 [============================>.] - ETA: 0s - loss: 0.3623\n",
      "Epoch 38: val_loss did not improve from 0.33432\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3631 - val_loss: 0.4042\n",
      "Epoch 39/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3687\n",
      "Epoch 39: val_loss improved from 0.33432 to 0.33172, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.3681 - val_loss: 0.3317\n",
      "Epoch 40/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3616\n",
      "Epoch 40: val_loss did not improve from 0.33172\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3622 - val_loss: 0.4780\n",
      "Epoch 41/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.3617\n",
      "Epoch 41: val_loss did not improve from 0.33172\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3618 - val_loss: 0.3634\n",
      "Epoch 42/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3596\n",
      "Epoch 42: val_loss did not improve from 0.33172\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3595 - val_loss: 0.3453\n",
      "Epoch 43/200\n",
      "533/547 [============================>.] - ETA: 0s - loss: 0.3605\n",
      "Epoch 43: val_loss did not improve from 0.33172\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3594 - val_loss: 0.3505\n",
      "Epoch 44/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3569\n",
      "Epoch 44: val_loss did not improve from 0.33172\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3566 - val_loss: 0.4659\n",
      "Epoch 45/200\n",
      "535/547 [============================>.] - ETA: 0s - loss: 0.3611\n",
      "Epoch 45: val_loss did not improve from 0.33172\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3618 - val_loss: 0.4261\n",
      "Epoch 46/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3578\n",
      "Epoch 46: val_loss did not improve from 0.33172\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3577 - val_loss: 0.3746\n",
      "Epoch 47/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3566\n",
      "Epoch 47: val_loss did not improve from 0.33172\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3575 - val_loss: 0.3569\n",
      "Epoch 48/200\n",
      "531/547 [============================>.] - ETA: 0s - loss: 0.3579\n",
      "Epoch 48: val_loss did not improve from 0.33172\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3575 - val_loss: 0.3342\n",
      "Epoch 49/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.3568\n",
      "Epoch 49: val_loss improved from 0.33172 to 0.32856, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.3568 - val_loss: 0.3286\n",
      "Epoch 50/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3540\n",
      "Epoch 50: val_loss improved from 0.32856 to 0.32825, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.3544 - val_loss: 0.3283\n",
      "Epoch 51/200\n",
      "542/547 [============================>.] - ETA: 0s - loss: 0.3517\n",
      "Epoch 51: val_loss did not improve from 0.32825\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3512 - val_loss: 0.3304\n",
      "Epoch 52/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3536\n",
      "Epoch 52: val_loss improved from 0.32825 to 0.32644, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.3531 - val_loss: 0.3264\n",
      "Epoch 53/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3535\n",
      "Epoch 53: val_loss did not improve from 0.32644\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3537 - val_loss: 0.3476\n",
      "Epoch 54/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3545\n",
      "Epoch 54: val_loss did not improve from 0.32644\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3540 - val_loss: 0.3275\n",
      "Epoch 55/200\n",
      "535/547 [============================>.] - ETA: 0s - loss: 0.3495\n",
      "Epoch 55: val_loss improved from 0.32644 to 0.32601, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.3505 - val_loss: 0.3260\n",
      "Epoch 56/200\n",
      "531/547 [============================>.] - ETA: 0s - loss: 0.3520\n",
      "Epoch 56: val_loss did not improve from 0.32601\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3516 - val_loss: 0.3373\n",
      "Epoch 57/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.3480\n",
      "Epoch 57: val_loss did not improve from 0.32601\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3482 - val_loss: 0.3331\n",
      "Epoch 58/200\n",
      "531/547 [============================>.] - ETA: 0s - loss: 0.3464\n",
      "Epoch 58: val_loss did not improve from 0.32601\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3473 - val_loss: 0.3517\n",
      "Epoch 59/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3503\n",
      "Epoch 59: val_loss did not improve from 0.32601\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3506 - val_loss: 0.3369\n",
      "Epoch 60/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3476\n",
      "Epoch 60: val_loss did not improve from 0.32601\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3474 - val_loss: 0.3735\n",
      "Epoch 61/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3494\n",
      "Epoch 61: val_loss did not improve from 0.32601\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3495 - val_loss: 0.3481\n",
      "Epoch 62/200\n",
      "542/547 [============================>.] - ETA: 0s - loss: 0.3481\n",
      "Epoch 62: val_loss did not improve from 0.32601\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3477 - val_loss: 0.3794\n",
      "Epoch 63/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3417\n",
      "Epoch 63: val_loss improved from 0.32601 to 0.32342, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.3422 - val_loss: 0.3234\n",
      "Epoch 64/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3445\n",
      "Epoch 64: val_loss did not improve from 0.32342\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3444 - val_loss: 0.3366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200\n",
      "535/547 [============================>.] - ETA: 0s - loss: 0.3452\n",
      "Epoch 65: val_loss did not improve from 0.32342\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3452 - val_loss: 0.3408\n",
      "Epoch 66/200\n",
      "542/547 [============================>.] - ETA: 0s - loss: 0.3433\n",
      "Epoch 66: val_loss did not improve from 0.32342\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3433 - val_loss: 0.3604\n",
      "Epoch 67/200\n",
      "542/547 [============================>.] - ETA: 0s - loss: 0.3461\n",
      "Epoch 67: val_loss did not improve from 0.32342\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3464 - val_loss: 0.3269\n",
      "Epoch 68/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3442\n",
      "Epoch 68: val_loss did not improve from 0.32342\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3434 - val_loss: 0.3459\n",
      "Epoch 69/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3425\n",
      "Epoch 69: val_loss improved from 0.32342 to 0.32112, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.3429 - val_loss: 0.3211\n",
      "Epoch 70/200\n",
      "535/547 [============================>.] - ETA: 0s - loss: 0.3447\n",
      "Epoch 70: val_loss did not improve from 0.32112\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3442 - val_loss: 0.3298\n",
      "Epoch 71/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.3407\n",
      "Epoch 71: val_loss did not improve from 0.32112\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3403 - val_loss: 0.3308\n",
      "Epoch 72/200\n",
      "533/547 [============================>.] - ETA: 0s - loss: 0.3412\n",
      "Epoch 72: val_loss did not improve from 0.32112\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3421 - val_loss: 0.3377\n",
      "Epoch 73/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3413\n",
      "Epoch 73: val_loss did not improve from 0.32112\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3411 - val_loss: 0.3403\n",
      "Epoch 74/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3406\n",
      "Epoch 74: val_loss improved from 0.32112 to 0.31938, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.3408 - val_loss: 0.3194\n",
      "Epoch 75/200\n",
      "534/547 [============================>.] - ETA: 0s - loss: 0.3411\n",
      "Epoch 75: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3403 - val_loss: 0.4632\n",
      "Epoch 76/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3388\n",
      "Epoch 76: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3385 - val_loss: 0.3306\n",
      "Epoch 77/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3393\n",
      "Epoch 77: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3393 - val_loss: 0.3211\n",
      "Epoch 78/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3386\n",
      "Epoch 78: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3386 - val_loss: 0.3357\n",
      "Epoch 79/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3380\n",
      "Epoch 79: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3377 - val_loss: 0.3528\n",
      "Epoch 80/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3382\n",
      "Epoch 80: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3382 - val_loss: 0.3693\n",
      "Epoch 81/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.3392\n",
      "Epoch 81: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3391 - val_loss: 0.3197\n",
      "Epoch 82/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.3404\n",
      "Epoch 82: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3397 - val_loss: 0.3221\n",
      "Epoch 83/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3370\n",
      "Epoch 83: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3372 - val_loss: 0.3225\n",
      "Epoch 84/200\n",
      "530/547 [============================>.] - ETA: 0s - loss: 0.3362\n",
      "Epoch 84: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3356 - val_loss: 0.3195\n",
      "Epoch 85/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.3346\n",
      "Epoch 85: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3342 - val_loss: 0.3352\n",
      "Epoch 86/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3351\n",
      "Epoch 86: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3348 - val_loss: 0.3278\n",
      "Epoch 87/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3359\n",
      "Epoch 87: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3357 - val_loss: 0.3594\n",
      "Epoch 88/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3341\n",
      "Epoch 88: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3343 - val_loss: 0.3493\n",
      "Epoch 89/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3343\n",
      "Epoch 89: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3341 - val_loss: 0.3279\n",
      "Epoch 90/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.3319\n",
      "Epoch 90: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3318 - val_loss: 0.3226\n",
      "Epoch 91/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3314\n",
      "Epoch 91: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3314 - val_loss: 0.3213\n",
      "Epoch 92/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3327\n",
      "Epoch 92: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3317 - val_loss: 0.3220\n",
      "Epoch 93/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3350\n",
      "Epoch 93: val_loss did not improve from 0.31938\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3339 - val_loss: 0.3272\n",
      "Epoch 94/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3324\n",
      "Epoch 94: val_loss improved from 0.31938 to 0.31769, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.3328 - val_loss: 0.3177\n",
      "Epoch 95/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3308\n",
      "Epoch 95: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3306 - val_loss: 0.3187\n",
      "Epoch 96/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3329\n",
      "Epoch 96: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3329 - val_loss: 0.3217\n",
      "Epoch 97/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.3296\n",
      "Epoch 97: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3301 - val_loss: 0.3283\n",
      "Epoch 98/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.3293\n",
      "Epoch 98: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3296 - val_loss: 0.4318\n",
      "Epoch 99/200\n",
      "535/547 [============================>.] - ETA: 0s - loss: 0.3287\n",
      "Epoch 99: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3289 - val_loss: 0.3979\n",
      "Epoch 100/200\n",
      "535/547 [============================>.] - ETA: 0s - loss: 0.3292\n",
      "Epoch 100: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3297 - val_loss: 0.3189\n",
      "Epoch 101/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3281\n",
      "Epoch 101: val_loss did not improve from 0.31769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3284 - val_loss: 0.3888\n",
      "Epoch 102/200\n",
      "529/547 [============================>.] - ETA: 0s - loss: 0.3280\n",
      "Epoch 102: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3278 - val_loss: 0.3267\n",
      "Epoch 103/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.3297\n",
      "Epoch 103: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3299 - val_loss: 0.3239\n",
      "Epoch 104/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3292\n",
      "Epoch 104: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3290 - val_loss: 0.3397\n",
      "Epoch 105/200\n",
      "534/547 [============================>.] - ETA: 0s - loss: 0.3251\n",
      "Epoch 105: val_loss did not improve from 0.31769\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3264 - val_loss: 0.3521\n",
      "Epoch 106/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3254\n",
      "Epoch 106: val_loss improved from 0.31769 to 0.31755, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.3255 - val_loss: 0.3175\n",
      "Epoch 107/200\n",
      "542/547 [============================>.] - ETA: 0s - loss: 0.3256\n",
      "Epoch 107: val_loss did not improve from 0.31755\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3257 - val_loss: 0.3501\n",
      "Epoch 108/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.3264\n",
      "Epoch 108: val_loss did not improve from 0.31755\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3264 - val_loss: 0.3504\n",
      "Epoch 109/200\n",
      "542/547 [============================>.] - ETA: 0s - loss: 0.3225\n",
      "Epoch 109: val_loss did not improve from 0.31755\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3232 - val_loss: 0.4092\n",
      "Epoch 110/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3226\n",
      "Epoch 110: val_loss did not improve from 0.31755\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3228 - val_loss: 0.3546\n",
      "Epoch 111/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3265\n",
      "Epoch 111: val_loss did not improve from 0.31755\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3263 - val_loss: 0.3191\n",
      "Epoch 112/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3219\n",
      "Epoch 112: val_loss did not improve from 0.31755\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3216 - val_loss: 0.4022\n",
      "Epoch 113/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.3230\n",
      "Epoch 113: val_loss did not improve from 0.31755\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3222 - val_loss: 0.3737\n",
      "Epoch 114/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3242\n",
      "Epoch 114: val_loss improved from 0.31755 to 0.31720, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 6ms/step - loss: 0.3237 - val_loss: 0.3172\n",
      "Epoch 115/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3226\n",
      "Epoch 115: val_loss did not improve from 0.31720\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3222 - val_loss: 0.3200\n",
      "Epoch 116/200\n",
      "534/547 [============================>.] - ETA: 0s - loss: 0.3251\n",
      "Epoch 116: val_loss did not improve from 0.31720\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3249 - val_loss: 0.3252\n",
      "Epoch 117/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.3211\n",
      "Epoch 117: val_loss did not improve from 0.31720\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3210 - val_loss: 0.3514\n",
      "Epoch 118/200\n",
      "533/547 [============================>.] - ETA: 0s - loss: 0.3199\n",
      "Epoch 118: val_loss did not improve from 0.31720\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3197 - val_loss: 0.3681\n",
      "Epoch 119/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.3204\n",
      "Epoch 119: val_loss did not improve from 0.31720\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3202 - val_loss: 0.3704\n",
      "Epoch 120/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3219\n",
      "Epoch 120: val_loss improved from 0.31720 to 0.31480, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.3219 - val_loss: 0.3148\n",
      "Epoch 121/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3203\n",
      "Epoch 121: val_loss did not improve from 0.31480\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3198 - val_loss: 0.3313\n",
      "Epoch 122/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3187\n",
      "Epoch 122: val_loss did not improve from 0.31480\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3187 - val_loss: 0.3214\n",
      "Epoch 123/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3173\n",
      "Epoch 123: val_loss did not improve from 0.31480\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3180 - val_loss: 0.3235\n",
      "Epoch 124/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3197\n",
      "Epoch 124: val_loss did not improve from 0.31480\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3193 - val_loss: 0.3473\n",
      "Epoch 125/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3189\n",
      "Epoch 125: val_loss did not improve from 0.31480\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3192 - val_loss: 0.3181\n",
      "Epoch 126/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3172\n",
      "Epoch 126: val_loss did not improve from 0.31480\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3180 - val_loss: 0.3289\n",
      "Epoch 127/200\n",
      "533/547 [============================>.] - ETA: 0s - loss: 0.3201\n",
      "Epoch 127: val_loss improved from 0.31480 to 0.31419, saving model to ./models\\checkpoint\n",
      "INFO:tensorflow:Assets written to: ./models\\checkpoint\\assets\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.3198 - val_loss: 0.3142\n",
      "Epoch 128/200\n",
      "530/547 [============================>.] - ETA: 0s - loss: 0.3163\n",
      "Epoch 128: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 3ms/step - loss: 0.3166 - val_loss: 0.3436\n",
      "Epoch 129/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3173\n",
      "Epoch 129: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3170 - val_loss: 0.3210\n",
      "Epoch 130/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3162\n",
      "Epoch 130: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3165 - val_loss: 0.3324\n",
      "Epoch 131/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3178\n",
      "Epoch 131: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3186 - val_loss: 0.3531\n",
      "Epoch 132/200\n",
      "534/547 [============================>.] - ETA: 0s - loss: 0.3167\n",
      "Epoch 132: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3176 - val_loss: 0.3698\n",
      "Epoch 133/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3170\n",
      "Epoch 133: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3163 - val_loss: 0.3373\n",
      "Epoch 134/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.3155\n",
      "Epoch 134: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3154 - val_loss: 0.3184\n",
      "Epoch 135/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3196\n",
      "Epoch 135: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3186 - val_loss: 0.3187\n",
      "Epoch 136/200\n",
      "534/547 [============================>.] - ETA: 0s - loss: 0.3139\n",
      "Epoch 136: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3141 - val_loss: 0.3629\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532/547 [============================>.] - ETA: 0s - loss: 0.3130\n",
      "Epoch 137: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3124 - val_loss: 0.3394\n",
      "Epoch 138/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3159\n",
      "Epoch 138: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3159 - val_loss: 0.3192\n",
      "Epoch 139/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3147\n",
      "Epoch 139: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3146 - val_loss: 0.3203\n",
      "Epoch 140/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3148\n",
      "Epoch 140: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3143 - val_loss: 0.3979\n",
      "Epoch 141/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.3125\n",
      "Epoch 141: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3126 - val_loss: 0.3273\n",
      "Epoch 142/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3140\n",
      "Epoch 142: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3141 - val_loss: 0.3252\n",
      "Epoch 143/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.3114\n",
      "Epoch 143: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3117 - val_loss: 0.3183\n",
      "Epoch 144/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.3106\n",
      "Epoch 144: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3107 - val_loss: 0.3162\n",
      "Epoch 145/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3126\n",
      "Epoch 145: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3126 - val_loss: 0.3870\n",
      "Epoch 146/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3093\n",
      "Epoch 146: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3093 - val_loss: 0.3152\n",
      "Epoch 147/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3112\n",
      "Epoch 147: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3112 - val_loss: 0.3238\n",
      "Epoch 148/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3086\n",
      "Epoch 148: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3089 - val_loss: 0.3215\n",
      "Epoch 149/200\n",
      "534/547 [============================>.] - ETA: 0s - loss: 0.3081\n",
      "Epoch 149: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3093 - val_loss: 0.3274\n",
      "Epoch 150/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3142\n",
      "Epoch 150: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3132 - val_loss: 0.3250\n",
      "Epoch 151/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3101\n",
      "Epoch 151: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3098 - val_loss: 0.3402\n",
      "Epoch 152/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.3101\n",
      "Epoch 152: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3080 - val_loss: 0.3457\n",
      "Epoch 153/200\n",
      "531/547 [============================>.] - ETA: 0s - loss: 0.3063\n",
      "Epoch 153: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3054 - val_loss: 0.3200\n",
      "Epoch 154/200\n",
      "534/547 [============================>.] - ETA: 0s - loss: 0.3070\n",
      "Epoch 154: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3079 - val_loss: 0.3171\n",
      "Epoch 155/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.3050\n",
      "Epoch 155: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3054 - val_loss: 0.3222\n",
      "Epoch 156/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3061\n",
      "Epoch 156: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3073 - val_loss: 0.3495\n",
      "Epoch 157/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3090\n",
      "Epoch 157: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3090 - val_loss: 0.3191\n",
      "Epoch 158/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.3072\n",
      "Epoch 158: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3069 - val_loss: 0.3414\n",
      "Epoch 159/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.3061\n",
      "Epoch 159: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3056 - val_loss: 0.3561\n",
      "Epoch 160/200\n",
      "535/547 [============================>.] - ETA: 0s - loss: 0.3050\n",
      "Epoch 160: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3055 - val_loss: 0.3406\n",
      "Epoch 161/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.3044\n",
      "Epoch 161: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3047 - val_loss: 0.3292\n",
      "Epoch 162/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.3045\n",
      "Epoch 162: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3045 - val_loss: 0.3292\n",
      "Epoch 163/200\n",
      "547/547 [==============================] - ETA: 0s - loss: 0.3052\n",
      "Epoch 163: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3052 - val_loss: 0.3149\n",
      "Epoch 164/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.3037\n",
      "Epoch 164: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3034 - val_loss: 0.3350\n",
      "Epoch 165/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3044\n",
      "Epoch 165: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3038 - val_loss: 0.4020\n",
      "Epoch 166/200\n",
      "542/547 [============================>.] - ETA: 0s - loss: 0.3026\n",
      "Epoch 166: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3027 - val_loss: 0.3415\n",
      "Epoch 167/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.3022\n",
      "Epoch 167: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3026 - val_loss: 0.3198\n",
      "Epoch 168/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.3048\n",
      "Epoch 168: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3047 - val_loss: 0.3163\n",
      "Epoch 169/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.2996\n",
      "Epoch 169: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2999 - val_loss: 0.3695\n",
      "Epoch 170/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.3001\n",
      "Epoch 170: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2997 - val_loss: 0.3365\n",
      "Epoch 171/200\n",
      "546/547 [============================>.] - ETA: 0s - loss: 0.3015\n",
      "Epoch 171: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3014 - val_loss: 0.3173\n",
      "Epoch 172/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.3002\n",
      "Epoch 172: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3004 - val_loss: 0.3690\n",
      "Epoch 173/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3021\n",
      "Epoch 173: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3017 - val_loss: 0.3498\n",
      "Epoch 174/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.3003\n",
      "Epoch 174: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3005 - val_loss: 0.3146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.3000\n",
      "Epoch 175: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2996 - val_loss: 0.3278\n",
      "Epoch 176/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.3015\n",
      "Epoch 176: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.3019 - val_loss: 0.3216\n",
      "Epoch 177/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.2987\n",
      "Epoch 177: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2989 - val_loss: 0.3170\n",
      "Epoch 178/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.2975\n",
      "Epoch 178: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2986 - val_loss: 0.3389\n",
      "Epoch 179/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.2965\n",
      "Epoch 179: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2960 - val_loss: 0.3470\n",
      "Epoch 180/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.2989\n",
      "Epoch 180: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2988 - val_loss: 0.3168\n",
      "Epoch 181/200\n",
      "540/547 [============================>.] - ETA: 0s - loss: 0.2952\n",
      "Epoch 181: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2957 - val_loss: 0.3251\n",
      "Epoch 182/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.2992\n",
      "Epoch 182: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2990 - val_loss: 0.3376\n",
      "Epoch 183/200\n",
      "543/547 [============================>.] - ETA: 0s - loss: 0.2987\n",
      "Epoch 183: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2984 - val_loss: 0.3175\n",
      "Epoch 184/200\n",
      "531/547 [============================>.] - ETA: 0s - loss: 0.2976\n",
      "Epoch 184: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2972 - val_loss: 0.3338\n",
      "Epoch 185/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.2960\n",
      "Epoch 185: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2949 - val_loss: 0.3315\n",
      "Epoch 186/200\n",
      "528/547 [===========================>..] - ETA: 0s - loss: 0.2961\n",
      "Epoch 186: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2959 - val_loss: 0.3397\n",
      "Epoch 187/200\n",
      "539/547 [============================>.] - ETA: 0s - loss: 0.2909\n",
      "Epoch 187: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2913 - val_loss: 0.3204\n",
      "Epoch 188/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.2931\n",
      "Epoch 188: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2923 - val_loss: 0.3319\n",
      "Epoch 189/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.2928\n",
      "Epoch 189: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2922 - val_loss: 0.3435\n",
      "Epoch 190/200\n",
      "537/547 [============================>.] - ETA: 0s - loss: 0.2922\n",
      "Epoch 190: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2920 - val_loss: 0.3186\n",
      "Epoch 191/200\n",
      "544/547 [============================>.] - ETA: 0s - loss: 0.2912\n",
      "Epoch 191: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2913 - val_loss: 0.4519\n",
      "Epoch 192/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.2915\n",
      "Epoch 192: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2921 - val_loss: 0.3235\n",
      "Epoch 193/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.2925\n",
      "Epoch 193: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2919 - val_loss: 0.3783\n",
      "Epoch 194/200\n",
      "545/547 [============================>.] - ETA: 0s - loss: 0.2889\n",
      "Epoch 194: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2891 - val_loss: 0.3192\n",
      "Epoch 195/200\n",
      "536/547 [============================>.] - ETA: 0s - loss: 0.2904\n",
      "Epoch 195: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2917 - val_loss: 0.3143\n",
      "Epoch 196/200\n",
      "541/547 [============================>.] - ETA: 0s - loss: 0.2906\n",
      "Epoch 196: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2908 - val_loss: 0.3629\n",
      "Epoch 197/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.2869\n",
      "Epoch 197: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2870 - val_loss: 0.3406\n",
      "Epoch 198/200\n",
      "535/547 [============================>.] - ETA: 0s - loss: 0.2882\n",
      "Epoch 198: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2892 - val_loss: 0.3479\n",
      "Epoch 199/200\n",
      "538/547 [============================>.] - ETA: 0s - loss: 0.2891\n",
      "Epoch 199: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2889 - val_loss: 0.3336\n",
      "Epoch 200/200\n",
      "532/547 [============================>.] - ETA: 0s - loss: 0.2874\n",
      "Epoch 200: val_loss did not improve from 0.31419\n",
      "547/547 [==============================] - 2s 4ms/step - loss: 0.2877 - val_loss: 0.3426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2582154ffd0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=SGD(momentum=0.02), loss=BinaryCrossentropy())\n",
    "\n",
    "model.fit(x=train_x,\n",
    "         y=train_y,\n",
    "         batch_size=32,\n",
    "         epochs=200,\n",
    "         validation_split=0.3,\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b586030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1c315d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    if (row == 0):\n",
    "        return '+'\n",
    "    elif (row == 1) :\n",
    "        return '-'\n",
    "\n",
    "def remap(y):\n",
    "    res= y.argmax(axis=1)\n",
    "    df = pd.DataFrame(res, columns=['Predicted'])\n",
    "    df['Predicted'] = df['Predicted'].apply(change_value)\n",
    "    return df\n",
    "\n",
    "def predict_result(model, x):    \n",
    "    y_pred = model.predict(x)\n",
    "    return remap(y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72707a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_result(model, train_x)\n",
    "y_true = remap(train_y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6c823be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.87508\n",
      "f1: 0.8750799999999999\n"
     ]
    }
   ],
   "source": [
    "print(f\"acc: {accuracy_score(y_pred,y_true)}\")\n",
    "print(f\"f1: {f1_score(y_pred,y_true, average='micro')}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7385cbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x25821baec10>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d46c26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_result(model, test_x)\n",
    "y_true = remap(test_y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a562d1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.85748\n",
      "f1: 0.85748\n"
     ]
    }
   ],
   "source": [
    "print(f\"acc: {accuracy_score(y_pred,y_true)}\")\n",
    "print(f\"f1: {f1_score(y_pred,y_true, average='micro')}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5821b950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2581230bfa0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f447aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.to_csv('./train_x.csv', index=False)\n",
    "train_y.to_csv('./train_y.csv', index=False)\n",
    "test_x.to_csv('./test_x.csv', index=False)\n",
    "test_y.to_csv('./test_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d719e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
