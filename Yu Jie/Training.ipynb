{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36a766c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "\n",
    "tqdm.pandas()\n",
    "spacy.require_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95749cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2492f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_directory = '../processed_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3f6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('../processed_data/train.json')\n",
    "test_df = pd.read_json('../processed_data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d586b5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11041</td>\n",
       "      <td>The show is average. It doesn't make me laugh ...</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5342</td>\n",
       "      <td>Rachel, Jo, Hannah, Tina, Bradley and John are...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9255</td>\n",
       "      <td>First, I should mention that I really enjoyed ...</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7152</td>\n",
       "      <td>'Holes' was a GREAT movie. Disney made the rig...</td>\n",
       "      <td>9</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11383</td>\n",
       "      <td>This is a fine musical with a timeless score b...</td>\n",
       "      <td>7</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>2979</td>\n",
       "      <td>There really isn't much to say about this movi...</td>\n",
       "      <td>10</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>9462</td>\n",
       "      <td>Clean family oriented movie. I laughed, I crie...</td>\n",
       "      <td>9</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>3995</td>\n",
       "      <td>'1408' is the latest hodge podge of cheap scar...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>11960</td>\n",
       "      <td>\"Scoop\" is also the name of a late-Thirties Ev...</td>\n",
       "      <td>8</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>10722</td>\n",
       "      <td>**SPOILERS*** Slow as molasses mummy movie inv...</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  rating label\n",
       "0      11041  The show is average. It doesn't make me laugh ...       4     -\n",
       "1       5342  Rachel, Jo, Hannah, Tina, Bradley and John are...       1     -\n",
       "2       9255  First, I should mention that I really enjoyed ...       2     -\n",
       "3       7152  'Holes' was a GREAT movie. Disney made the rig...       9     +\n",
       "4      11383  This is a fine musical with a timeless score b...       7     +\n",
       "...      ...                                                ...     ...   ...\n",
       "24995   2979  There really isn't much to say about this movi...      10     +\n",
       "24996   9462  Clean family oriented movie. I laughed, I crie...       9     +\n",
       "24997   3995  '1408' is the latest hodge podge of cheap scar...       1     -\n",
       "24998  11960  \"Scoop\" is also the name of a late-Thirties Ev...       8     +\n",
       "24999  10722  **SPOILERS*** Slow as molasses mummy movie inv...       4     -\n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7edcbbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>10</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000</td>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>7</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>9</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002</td>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>8</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10003</td>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>8</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>9998</td>\n",
       "      <td>I occasionally let my kids watch this garbage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>9999</td>\n",
       "      <td>When all we have anymore is pretty much realit...</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>999</td>\n",
       "      <td>The basic genre is a thriller intercut with an...</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>99</td>\n",
       "      <td>Four things intrigued me as to this film - fir...</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>9</td>\n",
       "      <td>David Bryce's comments nearby are exceptionall...</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  rating label\n",
       "0          0  I went and saw this movie last night after bei...      10     +\n",
       "1      10000  Actor turned director Bill Paxton follows up h...       7     +\n",
       "2      10001  As a recreational golfer with some knowledge o...       9     +\n",
       "3      10002  I saw this film in a sneak preview, and it is ...       8     +\n",
       "4      10003  Bill Paxton has taken the true story of the 19...       8     +\n",
       "...      ...                                                ...     ...   ...\n",
       "24995   9998  I occasionally let my kids watch this garbage ...       1     -\n",
       "24996   9999  When all we have anymore is pretty much realit...       1     -\n",
       "24997    999  The basic genre is a thriller intercut with an...       3     -\n",
       "24998     99  Four things intrigued me as to this film - fir...       3     -\n",
       "24999      9  David Bryce's comments nearby are exceptionall...       4     -\n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6eff201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1559ac28be0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1559ac65160>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1559471e820>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1559acf70c0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1559acfcf40>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1559471e7b0>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e76b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    return doc.vector\n",
    "\n",
    "def get_features(df):\n",
    "    text_list = df['text'].to_list()\n",
    "    x = []\n",
    "    for doc in tqdm_notebook(nlp.pipe(text_list, disable=[\"tagger\", \"parser\", \"lemmatizer\", 'attribute_ruler', 'ner'])):\n",
    "        x.append(doc.vector)\n",
    "    x = pd.DataFrame(x)\n",
    "    y = df['label']\n",
    "    y = pd.get_dummies(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b73ed6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f10bed177a410f9c495083cb9f1274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca5094f9e664ae9927a9babd7ef02a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x, train_y = get_features(train_df)\n",
    "test_x, test_y = get_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0a04d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.astype('float32')\n",
    "test_x = test_x.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9e92a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.098653</td>\n",
       "      <td>0.237332</td>\n",
       "      <td>-0.227549</td>\n",
       "      <td>-0.059897</td>\n",
       "      <td>0.041167</td>\n",
       "      <td>0.053954</td>\n",
       "      <td>0.041598</td>\n",
       "      <td>-0.212204</td>\n",
       "      <td>-0.030462</td>\n",
       "      <td>2.288359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080512</td>\n",
       "      <td>0.013579</td>\n",
       "      <td>-0.020111</td>\n",
       "      <td>-0.011932</td>\n",
       "      <td>0.052879</td>\n",
       "      <td>0.042916</td>\n",
       "      <td>-0.025879</td>\n",
       "      <td>-0.042477</td>\n",
       "      <td>0.055505</td>\n",
       "      <td>0.032498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.056069</td>\n",
       "      <td>0.164208</td>\n",
       "      <td>-0.111401</td>\n",
       "      <td>-0.112217</td>\n",
       "      <td>0.082695</td>\n",
       "      <td>0.010604</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>-0.160687</td>\n",
       "      <td>-0.034788</td>\n",
       "      <td>2.044537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165167</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>-0.042601</td>\n",
       "      <td>-0.053598</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>-0.003979</td>\n",
       "      <td>-0.089736</td>\n",
       "      <td>0.024499</td>\n",
       "      <td>0.031012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.025594</td>\n",
       "      <td>0.173807</td>\n",
       "      <td>-0.113592</td>\n",
       "      <td>-0.080288</td>\n",
       "      <td>0.121748</td>\n",
       "      <td>0.046220</td>\n",
       "      <td>0.017448</td>\n",
       "      <td>-0.121140</td>\n",
       "      <td>-0.023063</td>\n",
       "      <td>1.976007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.173515</td>\n",
       "      <td>0.041926</td>\n",
       "      <td>0.017024</td>\n",
       "      <td>-0.010830</td>\n",
       "      <td>0.033183</td>\n",
       "      <td>0.007571</td>\n",
       "      <td>-0.033585</td>\n",
       "      <td>-0.045075</td>\n",
       "      <td>-0.011756</td>\n",
       "      <td>0.023251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.026709</td>\n",
       "      <td>0.228059</td>\n",
       "      <td>-0.154029</td>\n",
       "      <td>-0.129077</td>\n",
       "      <td>0.087855</td>\n",
       "      <td>-0.003604</td>\n",
       "      <td>0.082640</td>\n",
       "      <td>-0.213129</td>\n",
       "      <td>-0.046223</td>\n",
       "      <td>2.200672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098601</td>\n",
       "      <td>0.031794</td>\n",
       "      <td>-0.006471</td>\n",
       "      <td>-0.035367</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>-0.073214</td>\n",
       "      <td>-0.107157</td>\n",
       "      <td>-0.002758</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.104389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016726</td>\n",
       "      <td>0.186435</td>\n",
       "      <td>-0.135161</td>\n",
       "      <td>-0.096413</td>\n",
       "      <td>0.078521</td>\n",
       "      <td>0.070652</td>\n",
       "      <td>0.035408</td>\n",
       "      <td>-0.179566</td>\n",
       "      <td>-0.033138</td>\n",
       "      <td>1.932208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181982</td>\n",
       "      <td>0.011574</td>\n",
       "      <td>0.040958</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.036744</td>\n",
       "      <td>0.055756</td>\n",
       "      <td>-0.035302</td>\n",
       "      <td>-0.084084</td>\n",
       "      <td>-0.013650</td>\n",
       "      <td>0.137881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>-0.048090</td>\n",
       "      <td>0.150734</td>\n",
       "      <td>-0.123009</td>\n",
       "      <td>-0.092647</td>\n",
       "      <td>0.115555</td>\n",
       "      <td>0.020735</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>-0.114092</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>1.860095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152963</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>-0.026846</td>\n",
       "      <td>-0.014160</td>\n",
       "      <td>0.028144</td>\n",
       "      <td>0.015263</td>\n",
       "      <td>-0.062839</td>\n",
       "      <td>-0.095883</td>\n",
       "      <td>-0.006639</td>\n",
       "      <td>0.062071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>-0.017593</td>\n",
       "      <td>0.180526</td>\n",
       "      <td>-0.135005</td>\n",
       "      <td>-0.160613</td>\n",
       "      <td>0.102259</td>\n",
       "      <td>0.020344</td>\n",
       "      <td>0.046305</td>\n",
       "      <td>-0.236645</td>\n",
       "      <td>0.012075</td>\n",
       "      <td>2.176674</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132983</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>-0.024606</td>\n",
       "      <td>-0.018269</td>\n",
       "      <td>0.093126</td>\n",
       "      <td>-0.020145</td>\n",
       "      <td>-0.034903</td>\n",
       "      <td>-0.032044</td>\n",
       "      <td>0.034915</td>\n",
       "      <td>0.105352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.137014</td>\n",
       "      <td>-0.122286</td>\n",
       "      <td>-0.069232</td>\n",
       "      <td>0.090511</td>\n",
       "      <td>0.021604</td>\n",
       "      <td>-0.009902</td>\n",
       "      <td>-0.085119</td>\n",
       "      <td>0.006140</td>\n",
       "      <td>2.144040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190512</td>\n",
       "      <td>0.064347</td>\n",
       "      <td>-0.009885</td>\n",
       "      <td>-0.021734</td>\n",
       "      <td>0.027786</td>\n",
       "      <td>0.033115</td>\n",
       "      <td>-0.042924</td>\n",
       "      <td>-0.037416</td>\n",
       "      <td>0.026347</td>\n",
       "      <td>0.049002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>-0.032313</td>\n",
       "      <td>0.183513</td>\n",
       "      <td>-0.095211</td>\n",
       "      <td>-0.022576</td>\n",
       "      <td>0.099779</td>\n",
       "      <td>0.036688</td>\n",
       "      <td>0.028060</td>\n",
       "      <td>-0.141543</td>\n",
       "      <td>-0.052280</td>\n",
       "      <td>1.960207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123420</td>\n",
       "      <td>0.027996</td>\n",
       "      <td>0.019448</td>\n",
       "      <td>-0.013598</td>\n",
       "      <td>0.019727</td>\n",
       "      <td>0.016152</td>\n",
       "      <td>-0.016169</td>\n",
       "      <td>-0.075711</td>\n",
       "      <td>-0.018939</td>\n",
       "      <td>0.019662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>-0.001639</td>\n",
       "      <td>0.120283</td>\n",
       "      <td>-0.084706</td>\n",
       "      <td>-0.063833</td>\n",
       "      <td>0.061360</td>\n",
       "      <td>0.042077</td>\n",
       "      <td>-0.008926</td>\n",
       "      <td>-0.100593</td>\n",
       "      <td>-0.043038</td>\n",
       "      <td>1.939081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113291</td>\n",
       "      <td>0.018487</td>\n",
       "      <td>-0.026250</td>\n",
       "      <td>-0.059459</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>-0.023138</td>\n",
       "      <td>0.023410</td>\n",
       "      <td>-0.011980</td>\n",
       "      <td>-0.014129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.098653  0.237332 -0.227549 -0.059897  0.041167  0.053954  0.041598   \n",
       "1     -0.056069  0.164208 -0.111401 -0.112217  0.082695  0.010604  0.085113   \n",
       "2     -0.025594  0.173807 -0.113592 -0.080288  0.121748  0.046220  0.017448   \n",
       "3     -0.026709  0.228059 -0.154029 -0.129077  0.087855 -0.003604  0.082640   \n",
       "4      0.016726  0.186435 -0.135161 -0.096413  0.078521  0.070652  0.035408   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24995 -0.048090  0.150734 -0.123009 -0.092647  0.115555  0.020735  0.023961   \n",
       "24996 -0.017593  0.180526 -0.135005 -0.160613  0.102259  0.020344  0.046305   \n",
       "24997  0.001822  0.137014 -0.122286 -0.069232  0.090511  0.021604 -0.009902   \n",
       "24998 -0.032313  0.183513 -0.095211 -0.022576  0.099779  0.036688  0.028060   \n",
       "24999 -0.001639  0.120283 -0.084706 -0.063833  0.061360  0.042077 -0.008926   \n",
       "\n",
       "            7         8         9    ...       290       291       292  \\\n",
       "0     -0.212204 -0.030462  2.288359  ... -0.080512  0.013579 -0.020111   \n",
       "1     -0.160687 -0.034788  2.044537  ... -0.165167  0.016225 -0.042601   \n",
       "2     -0.121140 -0.023063  1.976007  ... -0.173515  0.041926  0.017024   \n",
       "3     -0.213129 -0.046223  2.200672  ... -0.098601  0.031794 -0.006471   \n",
       "4     -0.179566 -0.033138  1.932208  ... -0.181982  0.011574  0.040958   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "24995 -0.114092  0.011440  1.860095  ... -0.152963 -0.000891 -0.026846   \n",
       "24996 -0.236645  0.012075  2.176674  ... -0.132983  0.009750 -0.024606   \n",
       "24997 -0.085119  0.006140  2.144040  ... -0.190512  0.064347 -0.009885   \n",
       "24998 -0.141543 -0.052280  1.960207  ... -0.123420  0.027996  0.019448   \n",
       "24999 -0.100593 -0.043038  1.939081  ... -0.113291  0.018487 -0.026250   \n",
       "\n",
       "            293       294       295       296       297       298       299  \n",
       "0     -0.011932  0.052879  0.042916 -0.025879 -0.042477  0.055505  0.032498  \n",
       "1     -0.053598  0.078946  0.026528 -0.003979 -0.089736  0.024499  0.031012  \n",
       "2     -0.010830  0.033183  0.007571 -0.033585 -0.045075 -0.011756  0.023251  \n",
       "3     -0.035367  0.128100 -0.073214 -0.107157 -0.002758  0.005043  0.104389  \n",
       "4      0.000037  0.036744  0.055756 -0.035302 -0.084084 -0.013650  0.137881  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "24995 -0.014160  0.028144  0.015263 -0.062839 -0.095883 -0.006639  0.062071  \n",
       "24996 -0.018269  0.093126 -0.020145 -0.034903 -0.032044  0.034915  0.105352  \n",
       "24997 -0.021734  0.027786  0.033115 -0.042924 -0.037416  0.026347  0.049002  \n",
       "24998 -0.013598  0.019727  0.016152 -0.016169 -0.075711 -0.018939  0.019662  \n",
       "24999 -0.059459  0.010273  0.007492 -0.023138  0.023410 -0.011980 -0.014129  \n",
       "\n",
       "[25000 rows x 300 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d0f64ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>+</th>\n",
       "      <th>-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       +  -\n",
       "0      0  1\n",
       "1      0  1\n",
       "2      0  1\n",
       "3      1  0\n",
       "4      1  0\n",
       "...   .. ..\n",
       "24995  1  0\n",
       "24996  1  0\n",
       "24997  0  1\n",
       "24998  1  0\n",
       "24999  0  1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5bfeec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(512, input_shape=(300,), activation='relu'),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "callbacks=[\n",
    "    ModelCheckpoint(filepath='./models/checkpoint',\n",
    "                   save_best_only=True, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4143131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "128/137 [===========================>..] - ETA: 0s - loss: 0.2991\n",
      "Epoch 1: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.2954 - val_loss: 0.3515\n",
      "Epoch 2/300\n",
      "130/137 [===========================>..] - ETA: 0s - loss: 0.2910\n",
      "Epoch 2: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.2899 - val_loss: 0.3540\n",
      "Epoch 3/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2942\n",
      "Epoch 3: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.2946 - val_loss: 0.4109\n",
      "Epoch 4/300\n",
      "131/137 [===========================>..] - ETA: 0s - loss: 0.2827\n",
      "Epoch 4: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2830 - val_loss: 0.3266\n",
      "Epoch 5/300\n",
      "128/137 [===========================>..] - ETA: 0s - loss: 0.2944\n",
      "Epoch 5: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2914 - val_loss: 0.3313\n",
      "Epoch 6/300\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2951\n",
      "Epoch 6: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2951 - val_loss: 0.3539\n",
      "Epoch 7/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2889\n",
      "Epoch 7: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2888 - val_loss: 0.3384\n",
      "Epoch 8/300\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.2909\n",
      "Epoch 8: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2881 - val_loss: 0.3596\n",
      "Epoch 9/300\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2977\n",
      "Epoch 9: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2977 - val_loss: 0.3584\n",
      "Epoch 10/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2858\n",
      "Epoch 10: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2854 - val_loss: 0.3245\n",
      "Epoch 11/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2911\n",
      "Epoch 11: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2909 - val_loss: 0.3303\n",
      "Epoch 12/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2904\n",
      "Epoch 12: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2906 - val_loss: 0.4799\n",
      "Epoch 13/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2856\n",
      "Epoch 13: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2855 - val_loss: 0.3350\n",
      "Epoch 14/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2899\n",
      "Epoch 14: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2895 - val_loss: 0.3232\n",
      "Epoch 15/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2893\n",
      "Epoch 15: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2888 - val_loss: 0.3300\n",
      "Epoch 16/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2950\n",
      "Epoch 16: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2970 - val_loss: 0.3631\n",
      "Epoch 17/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2925\n",
      "Epoch 17: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2903 - val_loss: 0.4228\n",
      "Epoch 18/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2865\n",
      "Epoch 18: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2865 - val_loss: 0.3588\n",
      "Epoch 19/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2923\n",
      "Epoch 19: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2919 - val_loss: 0.3407\n",
      "Epoch 20/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2862\n",
      "Epoch 20: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2862 - val_loss: 0.3304\n",
      "Epoch 21/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2869\n",
      "Epoch 21: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2868 - val_loss: 0.3493\n",
      "Epoch 22/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2902\n",
      "Epoch 22: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2889 - val_loss: 0.4195\n",
      "Epoch 23/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2830\n",
      "Epoch 23: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2825 - val_loss: 0.3286\n",
      "Epoch 24/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2906\n",
      "Epoch 24: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2896 - val_loss: 0.3362\n",
      "Epoch 25/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2855\n",
      "Epoch 25: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2856 - val_loss: 0.3404\n",
      "Epoch 26/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2788\n",
      "Epoch 26: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2795 - val_loss: 0.3446\n",
      "Epoch 27/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2914\n",
      "Epoch 27: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2906 - val_loss: 0.3544\n",
      "Epoch 28/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2943\n",
      "Epoch 28: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2940 - val_loss: 0.3391\n",
      "Epoch 29/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2874\n",
      "Epoch 29: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2886 - val_loss: 0.3350\n",
      "Epoch 30/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2755\n",
      "Epoch 30: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2759 - val_loss: 0.4189\n",
      "Epoch 31/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2900\n",
      "Epoch 31: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2889 - val_loss: 0.3274\n",
      "Epoch 32/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2918\n",
      "Epoch 32: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2902 - val_loss: 0.3354\n",
      "Epoch 33/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2747\n",
      "Epoch 33: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2741 - val_loss: 0.3390\n",
      "Epoch 34/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2863\n",
      "Epoch 34: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2875 - val_loss: 0.6613\n",
      "Epoch 35/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2786\n",
      "Epoch 35: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2799 - val_loss: 0.3777\n",
      "Epoch 36/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2915\n",
      "Epoch 36: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2918 - val_loss: 0.3470\n",
      "Epoch 37/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2840\n",
      "Epoch 37: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2837 - val_loss: 0.3307\n",
      "Epoch 38/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2766\n",
      "Epoch 38: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2766 - val_loss: 0.3473\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/137 [============================>.] - ETA: 0s - loss: 0.2749\n",
      "Epoch 39: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2753 - val_loss: 0.4612\n",
      "Epoch 40/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2707\n",
      "Epoch 40: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2710 - val_loss: 0.3381\n",
      "Epoch 41/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2865\n",
      "Epoch 41: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2871 - val_loss: 0.5205\n",
      "Epoch 42/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2872\n",
      "Epoch 42: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2868 - val_loss: 0.3551\n",
      "Epoch 43/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2783\n",
      "Epoch 43: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2777 - val_loss: 0.3558\n",
      "Epoch 44/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2809\n",
      "Epoch 44: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2808 - val_loss: 0.3879\n",
      "Epoch 45/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2879\n",
      "Epoch 45: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2882 - val_loss: 0.4444\n",
      "Epoch 46/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2871\n",
      "Epoch 46: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2871 - val_loss: 0.3541\n",
      "Epoch 47/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2735\n",
      "Epoch 47: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2726 - val_loss: 0.3762\n",
      "Epoch 48/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2808\n",
      "Epoch 48: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2812 - val_loss: 0.3348\n",
      "Epoch 49/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2776\n",
      "Epoch 49: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2792 - val_loss: 0.3724\n",
      "Epoch 50/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2757\n",
      "Epoch 50: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2745 - val_loss: 0.4291\n",
      "Epoch 51/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2840\n",
      "Epoch 51: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2844 - val_loss: 0.3602\n",
      "Epoch 52/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2701\n",
      "Epoch 52: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2741 - val_loss: 0.5171\n",
      "Epoch 53/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2810\n",
      "Epoch 53: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2815 - val_loss: 0.3252\n",
      "Epoch 54/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2739\n",
      "Epoch 54: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2738 - val_loss: 0.3264\n",
      "Epoch 55/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2827\n",
      "Epoch 55: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2829 - val_loss: 0.3634\n",
      "Epoch 56/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2879\n",
      "Epoch 56: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2881 - val_loss: 0.4861\n",
      "Epoch 57/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2838\n",
      "Epoch 57: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2824 - val_loss: 0.3575\n",
      "Epoch 58/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2749\n",
      "Epoch 58: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2741 - val_loss: 0.3271\n",
      "Epoch 59/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2715\n",
      "Epoch 59: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2712 - val_loss: 0.3260\n",
      "Epoch 60/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2745\n",
      "Epoch 60: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2743 - val_loss: 0.3302\n",
      "Epoch 61/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2749\n",
      "Epoch 61: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2763 - val_loss: 0.4662\n",
      "Epoch 62/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2710\n",
      "Epoch 62: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2710 - val_loss: 0.3818\n",
      "Epoch 63/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2737\n",
      "Epoch 63: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2725 - val_loss: 0.4632\n",
      "Epoch 64/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2732\n",
      "Epoch 64: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2727 - val_loss: 0.3377\n",
      "Epoch 65/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2755\n",
      "Epoch 65: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2746 - val_loss: 0.3643\n",
      "Epoch 66/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2799\n",
      "Epoch 66: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2790 - val_loss: 0.4114\n",
      "Epoch 67/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2746\n",
      "Epoch 67: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2735 - val_loss: 0.3599\n",
      "Epoch 68/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2707\n",
      "Epoch 68: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2691 - val_loss: 0.3361\n",
      "Epoch 69/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2811\n",
      "Epoch 69: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2806 - val_loss: 0.3259\n",
      "Epoch 70/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2627\n",
      "Epoch 70: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2625 - val_loss: 0.3381\n",
      "Epoch 71/300\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2678\n",
      "Epoch 71: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2678 - val_loss: 0.4075\n",
      "Epoch 72/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2789\n",
      "Epoch 72: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2798 - val_loss: 0.3273\n",
      "Epoch 73/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2760\n",
      "Epoch 73: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2756 - val_loss: 0.4357\n",
      "Epoch 74/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2716\n",
      "Epoch 74: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2713 - val_loss: 0.3490\n",
      "Epoch 75/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2687\n",
      "Epoch 75: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2680 - val_loss: 0.3524\n",
      "Epoch 76/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2811\n",
      "Epoch 76: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2805 - val_loss: 0.4100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2744\n",
      "Epoch 77: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2746 - val_loss: 0.3537\n",
      "Epoch 78/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2680\n",
      "Epoch 78: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2702 - val_loss: 0.3263\n",
      "Epoch 79/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2678\n",
      "Epoch 79: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2676 - val_loss: 0.3408\n",
      "Epoch 80/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2784\n",
      "Epoch 80: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2788 - val_loss: 0.3292\n",
      "Epoch 81/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2747\n",
      "Epoch 81: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2732 - val_loss: 0.3713\n",
      "Epoch 82/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2721\n",
      "Epoch 82: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2740 - val_loss: 0.5340\n",
      "Epoch 83/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2791\n",
      "Epoch 83: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2783 - val_loss: 0.3458\n",
      "Epoch 84/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2659\n",
      "Epoch 84: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2666 - val_loss: 0.4499\n",
      "Epoch 85/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2663\n",
      "Epoch 85: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2670 - val_loss: 0.3491\n",
      "Epoch 86/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2598\n",
      "Epoch 86: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2611 - val_loss: 0.3271\n",
      "Epoch 87/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2726\n",
      "Epoch 87: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2758 - val_loss: 0.3959\n",
      "Epoch 88/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2614\n",
      "Epoch 88: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2643 - val_loss: 0.3855\n",
      "Epoch 89/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2720\n",
      "Epoch 89: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2721 - val_loss: 0.4401\n",
      "Epoch 90/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2722\n",
      "Epoch 90: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2727 - val_loss: 0.3483\n",
      "Epoch 91/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2643\n",
      "Epoch 91: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2631 - val_loss: 0.3723\n",
      "Epoch 92/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2664\n",
      "Epoch 92: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2699 - val_loss: 0.4891\n",
      "Epoch 93/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2600\n",
      "Epoch 93: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2599 - val_loss: 0.3303\n",
      "Epoch 94/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2674\n",
      "Epoch 94: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2709 - val_loss: 0.4037\n",
      "Epoch 95/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2634\n",
      "Epoch 95: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2640 - val_loss: 0.4597\n",
      "Epoch 96/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2700\n",
      "Epoch 96: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2686 - val_loss: 0.3339\n",
      "Epoch 97/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2733\n",
      "Epoch 97: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2727 - val_loss: 0.3304\n",
      "Epoch 98/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2624\n",
      "Epoch 98: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2637 - val_loss: 0.4490\n",
      "Epoch 99/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2660\n",
      "Epoch 99: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2682 - val_loss: 0.4116\n",
      "Epoch 100/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2620\n",
      "Epoch 100: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2671 - val_loss: 0.4732\n",
      "Epoch 101/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2631\n",
      "Epoch 101: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2640 - val_loss: 0.3310\n",
      "Epoch 102/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2657\n",
      "Epoch 102: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2647 - val_loss: 0.3391\n",
      "Epoch 103/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2595\n",
      "Epoch 103: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2598 - val_loss: 0.3431\n",
      "Epoch 104/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2640\n",
      "Epoch 104: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2639 - val_loss: 0.4111\n",
      "Epoch 105/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2602\n",
      "Epoch 105: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2597 - val_loss: 0.3608\n",
      "Epoch 106/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2663\n",
      "Epoch 106: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2651 - val_loss: 0.3528\n",
      "Epoch 107/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2668\n",
      "Epoch 107: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2669 - val_loss: 0.5084\n",
      "Epoch 108/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2525\n",
      "Epoch 108: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2530 - val_loss: 0.5004\n",
      "Epoch 109/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2746\n",
      "Epoch 109: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2738 - val_loss: 0.3792\n",
      "Epoch 110/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2587\n",
      "Epoch 110: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2571 - val_loss: 0.4126\n",
      "Epoch 111/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2574\n",
      "Epoch 111: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2572 - val_loss: 0.3370\n",
      "Epoch 112/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2581\n",
      "Epoch 112: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2588 - val_loss: 0.3559\n",
      "Epoch 113/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2586\n",
      "Epoch 113: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2604 - val_loss: 0.7009\n",
      "Epoch 114/300\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2675\n",
      "Epoch 114: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2675 - val_loss: 0.3879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2535\n",
      "Epoch 115: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2522 - val_loss: 0.4448\n",
      "Epoch 116/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2567\n",
      "Epoch 116: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2555 - val_loss: 0.3434\n",
      "Epoch 117/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2592\n",
      "Epoch 117: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2606 - val_loss: 0.3349\n",
      "Epoch 118/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2507\n",
      "Epoch 118: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2510 - val_loss: 0.4195\n",
      "Epoch 119/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2556\n",
      "Epoch 119: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2543 - val_loss: 0.3562\n",
      "Epoch 120/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2658\n",
      "Epoch 120: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2661 - val_loss: 0.3597\n",
      "Epoch 121/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2563\n",
      "Epoch 121: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2556 - val_loss: 0.3543\n",
      "Epoch 122/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2574\n",
      "Epoch 122: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2568 - val_loss: 0.3979\n",
      "Epoch 123/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2544\n",
      "Epoch 123: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2545 - val_loss: 0.3390\n",
      "Epoch 124/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2622\n",
      "Epoch 124: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2636 - val_loss: 0.4778\n",
      "Epoch 125/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2551\n",
      "Epoch 125: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2545 - val_loss: 0.4252\n",
      "Epoch 126/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2483\n",
      "Epoch 126: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2482 - val_loss: 0.4033\n",
      "Epoch 127/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2606\n",
      "Epoch 127: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2602 - val_loss: 0.3601\n",
      "Epoch 128/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2683\n",
      "Epoch 128: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2691 - val_loss: 0.4021\n",
      "Epoch 129/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2566\n",
      "Epoch 129: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2574 - val_loss: 0.3315\n",
      "Epoch 130/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2467\n",
      "Epoch 130: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2475 - val_loss: 0.4720\n",
      "Epoch 131/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2448\n",
      "Epoch 131: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2461 - val_loss: 0.4171\n",
      "Epoch 132/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2615\n",
      "Epoch 132: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2598 - val_loss: 0.3496\n",
      "Epoch 133/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2464\n",
      "Epoch 133: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2476 - val_loss: 0.4211\n",
      "Epoch 134/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2563\n",
      "Epoch 134: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2552 - val_loss: 0.3903\n",
      "Epoch 135/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2580\n",
      "Epoch 135: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2574 - val_loss: 0.3473\n",
      "Epoch 136/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2500\n",
      "Epoch 136: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2506 - val_loss: 0.3644\n",
      "Epoch 137/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2547\n",
      "Epoch 137: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2552 - val_loss: 0.4684\n",
      "Epoch 138/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2574\n",
      "Epoch 138: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2567 - val_loss: 0.4104\n",
      "Epoch 139/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2567\n",
      "Epoch 139: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2568 - val_loss: 0.4036\n",
      "Epoch 140/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2533\n",
      "Epoch 140: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2526 - val_loss: 0.3518\n",
      "Epoch 141/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2510\n",
      "Epoch 141: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2518 - val_loss: 0.3559\n",
      "Epoch 142/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2604\n",
      "Epoch 142: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2603 - val_loss: 0.3408\n",
      "Epoch 143/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2555\n",
      "Epoch 143: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2549 - val_loss: 0.3532\n",
      "Epoch 144/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2430\n",
      "Epoch 144: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2423 - val_loss: 0.4805\n",
      "Epoch 145/300\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.2514\n",
      "Epoch 145: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2496 - val_loss: 0.4448\n",
      "Epoch 146/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2629\n",
      "Epoch 146: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2631 - val_loss: 0.4261\n",
      "Epoch 147/300\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.2453\n",
      "Epoch 147: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2422 - val_loss: 0.3431\n",
      "Epoch 148/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2466\n",
      "Epoch 148: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2456 - val_loss: 0.3774\n",
      "Epoch 149/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2484\n",
      "Epoch 149: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2487 - val_loss: 0.3696\n",
      "Epoch 150/300\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2477\n",
      "Epoch 150: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2477 - val_loss: 0.4111\n",
      "Epoch 151/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2496\n",
      "Epoch 151: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2494 - val_loss: 0.3827\n",
      "Epoch 152/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2352\n",
      "Epoch 152: val_loss did not improve from 0.32258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2352 - val_loss: 0.4439\n",
      "Epoch 153/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2426\n",
      "Epoch 153: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2462 - val_loss: 0.5256\n",
      "Epoch 154/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2386\n",
      "Epoch 154: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2386 - val_loss: 0.3750\n",
      "Epoch 155/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2521\n",
      "Epoch 155: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2526 - val_loss: 0.3778\n",
      "Epoch 156/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2455\n",
      "Epoch 156: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2449 - val_loss: 0.3463\n",
      "Epoch 157/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2425\n",
      "Epoch 157: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2437 - val_loss: 0.3799\n",
      "Epoch 158/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2530\n",
      "Epoch 158: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2526 - val_loss: 0.3498\n",
      "Epoch 159/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2437\n",
      "Epoch 159: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2435 - val_loss: 0.3333\n",
      "Epoch 160/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2407\n",
      "Epoch 160: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2400 - val_loss: 0.3373\n",
      "Epoch 161/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2405\n",
      "Epoch 161: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2398 - val_loss: 0.3518\n",
      "Epoch 162/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2385\n",
      "Epoch 162: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2388 - val_loss: 0.3829\n",
      "Epoch 163/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2446\n",
      "Epoch 163: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2431 - val_loss: 0.3660\n",
      "Epoch 164/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2316\n",
      "Epoch 164: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2322 - val_loss: 0.4097\n",
      "Epoch 165/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2464\n",
      "Epoch 165: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2494 - val_loss: 0.3507\n",
      "Epoch 166/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2405\n",
      "Epoch 166: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2405 - val_loss: 0.4492\n",
      "Epoch 167/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2457\n",
      "Epoch 167: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2479 - val_loss: 0.3503\n",
      "Epoch 168/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2387\n",
      "Epoch 168: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2383 - val_loss: 0.3392\n",
      "Epoch 169/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2523\n",
      "Epoch 169: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2524 - val_loss: 0.4141\n",
      "Epoch 170/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2345\n",
      "Epoch 170: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2331 - val_loss: 0.3623\n",
      "Epoch 171/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2416\n",
      "Epoch 171: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2414 - val_loss: 0.5524\n",
      "Epoch 172/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2444\n",
      "Epoch 172: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2456 - val_loss: 0.3343\n",
      "Epoch 173/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2357\n",
      "Epoch 173: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2351 - val_loss: 0.3428\n",
      "Epoch 174/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2369\n",
      "Epoch 174: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2392 - val_loss: 0.5761\n",
      "Epoch 175/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2358\n",
      "Epoch 175: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2358 - val_loss: 0.3435\n",
      "Epoch 176/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2415\n",
      "Epoch 176: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2403 - val_loss: 0.3959\n",
      "Epoch 177/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2451\n",
      "Epoch 177: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2458 - val_loss: 0.3826\n",
      "Epoch 178/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2291\n",
      "Epoch 178: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2297 - val_loss: 0.3676\n",
      "Epoch 179/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2429\n",
      "Epoch 179: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2430 - val_loss: 0.3821\n",
      "Epoch 180/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2333\n",
      "Epoch 180: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2351 - val_loss: 0.3662\n",
      "Epoch 181/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2317\n",
      "Epoch 181: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2310 - val_loss: 0.3792\n",
      "Epoch 182/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2434\n",
      "Epoch 182: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2430 - val_loss: 0.3425\n",
      "Epoch 183/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2375\n",
      "Epoch 183: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2366 - val_loss: 0.4153\n",
      "Epoch 184/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2450\n",
      "Epoch 184: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2447 - val_loss: 0.3502\n",
      "Epoch 185/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2344\n",
      "Epoch 185: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2334 - val_loss: 0.3537\n",
      "Epoch 186/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2347\n",
      "Epoch 186: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2346 - val_loss: 0.4943\n",
      "Epoch 187/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2311\n",
      "Epoch 187: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2313 - val_loss: 0.3448\n",
      "Epoch 188/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2303\n",
      "Epoch 188: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2289 - val_loss: 0.3764\n",
      "Epoch 189/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2342\n",
      "Epoch 189: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2325 - val_loss: 0.3785\n",
      "Epoch 190/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/137 [============================>.] - ETA: 0s - loss: 0.2393\n",
      "Epoch 190: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2383 - val_loss: 0.3636\n",
      "Epoch 191/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2204\n",
      "Epoch 191: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2291 - val_loss: 0.6379\n",
      "Epoch 192/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2325\n",
      "Epoch 192: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2363 - val_loss: 0.6075\n",
      "Epoch 193/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2278\n",
      "Epoch 193: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2292 - val_loss: 0.4266\n",
      "Epoch 194/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2327\n",
      "Epoch 194: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2348 - val_loss: 0.8134\n",
      "Epoch 195/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2326\n",
      "Epoch 195: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2324 - val_loss: 0.3722\n",
      "Epoch 196/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2301\n",
      "Epoch 196: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2305 - val_loss: 0.4419\n",
      "Epoch 197/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2331\n",
      "Epoch 197: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2322 - val_loss: 0.3612\n",
      "Epoch 198/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2377\n",
      "Epoch 198: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2361 - val_loss: 0.4096\n",
      "Epoch 199/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2232\n",
      "Epoch 199: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2216 - val_loss: 0.3481\n",
      "Epoch 200/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2222\n",
      "Epoch 200: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2229 - val_loss: 0.3546\n",
      "Epoch 201/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2273\n",
      "Epoch 201: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2267 - val_loss: 0.3596\n",
      "Epoch 202/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2352\n",
      "Epoch 202: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2341 - val_loss: 0.3861\n",
      "Epoch 203/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2256\n",
      "Epoch 203: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2242 - val_loss: 0.3720\n",
      "Epoch 204/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2226\n",
      "Epoch 204: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2218 - val_loss: 0.3849\n",
      "Epoch 205/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2267\n",
      "Epoch 205: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2256 - val_loss: 0.3647\n",
      "Epoch 206/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2298\n",
      "Epoch 206: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2316 - val_loss: 0.3974\n",
      "Epoch 207/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2267\n",
      "Epoch 207: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2276 - val_loss: 0.6254\n",
      "Epoch 208/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2241\n",
      "Epoch 208: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2233 - val_loss: 0.4353\n",
      "Epoch 209/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2164\n",
      "Epoch 209: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2172 - val_loss: 0.5713\n",
      "Epoch 210/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2412\n",
      "Epoch 210: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2397 - val_loss: 0.3607\n",
      "Epoch 211/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2222\n",
      "Epoch 211: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2214 - val_loss: 0.3713\n",
      "Epoch 212/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2343\n",
      "Epoch 212: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2337 - val_loss: 0.3523\n",
      "Epoch 213/300\n",
      "130/137 [===========================>..] - ETA: 0s - loss: 0.2280\n",
      "Epoch 213: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2254 - val_loss: 0.3885\n",
      "Epoch 214/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2304\n",
      "Epoch 214: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2298 - val_loss: 0.3619\n",
      "Epoch 215/300\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.2323\n",
      "Epoch 215: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.2304 - val_loss: 0.3932\n",
      "Epoch 216/300\n",
      "127/137 [==========================>...] - ETA: 0s - loss: 0.2153\n",
      "Epoch 216: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.2162 - val_loss: 0.5767\n",
      "Epoch 217/300\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.2315\n",
      "Epoch 217: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 4ms/step - loss: 0.2289 - val_loss: 0.4091\n",
      "Epoch 218/300\n",
      "130/137 [===========================>..] - ETA: 0s - loss: 0.2166\n",
      "Epoch 218: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2214 - val_loss: 0.4070\n",
      "Epoch 219/300\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2103\n",
      "Epoch 219: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2103 - val_loss: 0.3676\n",
      "Epoch 220/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2281\n",
      "Epoch 220: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2273 - val_loss: 0.3465\n",
      "Epoch 221/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2244\n",
      "Epoch 221: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2247 - val_loss: 0.3585\n",
      "Epoch 222/300\n",
      "131/137 [===========================>..] - ETA: 0s - loss: 0.2155\n",
      "Epoch 222: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2147 - val_loss: 0.3832\n",
      "Epoch 223/300\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.2255\n",
      "Epoch 223: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2190 - val_loss: 0.3671\n",
      "Epoch 224/300\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2283\n",
      "Epoch 224: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2283 - val_loss: 0.3534\n",
      "Epoch 225/300\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.2267\n",
      "Epoch 225: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.2256 - val_loss: 0.3852\n",
      "Epoch 226/300\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.2112\n",
      "Epoch 226: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2091 - val_loss: 0.3722\n",
      "Epoch 227/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2220\n",
      "Epoch 227: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2219 - val_loss: 0.4384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2217\n",
      "Epoch 228: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2204 - val_loss: 0.3689\n",
      "Epoch 229/300\n",
      "131/137 [===========================>..] - ETA: 0s - loss: 0.2280\n",
      "Epoch 229: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2300 - val_loss: 0.5210\n",
      "Epoch 230/300\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.2234\n",
      "Epoch 230: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2223 - val_loss: 0.3680\n",
      "Epoch 231/300\n",
      "131/137 [===========================>..] - ETA: 0s - loss: 0.2170\n",
      "Epoch 231: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2176 - val_loss: 0.4176\n",
      "Epoch 232/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2258\n",
      "Epoch 232: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2251 - val_loss: 0.3621\n",
      "Epoch 233/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2080\n",
      "Epoch 233: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.4045\n",
      "Epoch 234/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2227\n",
      "Epoch 234: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 6ms/step - loss: 0.2215 - val_loss: 0.3596\n",
      "Epoch 235/300\n",
      "137/137 [==============================] - ETA: 0s - loss: 0.2085\n",
      "Epoch 235: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2085 - val_loss: 0.5838\n",
      "Epoch 236/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2264\n",
      "Epoch 236: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2244 - val_loss: 0.3645\n",
      "Epoch 237/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2146\n",
      "Epoch 237: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2133 - val_loss: 0.3901\n",
      "Epoch 238/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2224\n",
      "Epoch 238: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2206 - val_loss: 0.3873\n",
      "Epoch 239/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2096\n",
      "Epoch 239: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2104 - val_loss: 0.3947\n",
      "Epoch 240/300\n",
      "129/137 [===========================>..] - ETA: 0s - loss: 0.2122\n",
      "Epoch 240: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2107 - val_loss: 0.4418\n",
      "Epoch 241/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2241\n",
      "Epoch 241: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2258 - val_loss: 0.3826\n",
      "Epoch 242/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2172\n",
      "Epoch 242: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2162 - val_loss: 0.4670\n",
      "Epoch 243/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2190\n",
      "Epoch 243: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2190 - val_loss: 0.6817\n",
      "Epoch 244/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2281\n",
      "Epoch 244: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2271 - val_loss: 0.3870\n",
      "Epoch 245/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1897\n",
      "Epoch 245: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1910 - val_loss: 0.3770\n",
      "Epoch 246/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2335\n",
      "Epoch 246: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2321 - val_loss: 0.3917\n",
      "Epoch 247/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2068\n",
      "Epoch 247: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.3670\n",
      "Epoch 248/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2082\n",
      "Epoch 248: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.3699\n",
      "Epoch 249/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2171\n",
      "Epoch 249: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2166 - val_loss: 0.3748\n",
      "Epoch 250/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2135\n",
      "Epoch 250: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2135 - val_loss: 0.3535\n",
      "Epoch 251/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2061\n",
      "Epoch 251: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.4524\n",
      "Epoch 252/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2080\n",
      "Epoch 252: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2084 - val_loss: 0.5448\n",
      "Epoch 253/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1991\n",
      "Epoch 253: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1995 - val_loss: 0.5411\n",
      "Epoch 254/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2050\n",
      "Epoch 254: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2077 - val_loss: 0.3860\n",
      "Epoch 255/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2257\n",
      "Epoch 255: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2240 - val_loss: 0.3607\n",
      "Epoch 256/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2107\n",
      "Epoch 256: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2109 - val_loss: 0.3572\n",
      "Epoch 257/300\n",
      "136/137 [============================>.] - ETA: 0s - loss: 0.2065\n",
      "Epoch 257: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2059 - val_loss: 0.4063\n",
      "Epoch 258/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.1969\n",
      "Epoch 258: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1965 - val_loss: 0.4536\n",
      "Epoch 259/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2091\n",
      "Epoch 259: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2083 - val_loss: 0.4560\n",
      "Epoch 260/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2187\n",
      "Epoch 260: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2186 - val_loss: 0.4178\n",
      "Epoch 261/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2046\n",
      "Epoch 261: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2029 - val_loss: 0.3827\n",
      "Epoch 262/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2125\n",
      "Epoch 262: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2107 - val_loss: 0.3952\n",
      "Epoch 263/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2082\n",
      "Epoch 263: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.3823\n",
      "Epoch 264/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2003\n",
      "Epoch 264: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1995 - val_loss: 0.3527\n",
      "Epoch 265/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1990\n",
      "Epoch 265: val_loss did not improve from 0.32258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1975 - val_loss: 0.3923\n",
      "Epoch 266/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2070\n",
      "Epoch 266: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2100 - val_loss: 1.0592\n",
      "Epoch 267/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2075\n",
      "Epoch 267: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.4895\n",
      "Epoch 268/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2079\n",
      "Epoch 268: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2103 - val_loss: 0.3751\n",
      "Epoch 269/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2081\n",
      "Epoch 269: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2073 - val_loss: 0.3775\n",
      "Epoch 270/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1913\n",
      "Epoch 270: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.8532\n",
      "Epoch 271/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2124\n",
      "Epoch 271: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2109 - val_loss: 0.3806\n",
      "Epoch 272/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 272: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1863 - val_loss: 0.3990\n",
      "Epoch 273/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2307\n",
      "Epoch 273: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2293 - val_loss: 0.4818\n",
      "Epoch 274/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1959\n",
      "Epoch 274: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1979 - val_loss: 0.4529\n",
      "Epoch 275/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1977\n",
      "Epoch 275: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1962 - val_loss: 0.4521\n",
      "Epoch 276/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2094\n",
      "Epoch 276: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2080 - val_loss: 0.3980\n",
      "Epoch 277/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 277: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1888 - val_loss: 0.6106\n",
      "Epoch 278/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2021\n",
      "Epoch 278: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.4385\n",
      "Epoch 279/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2005\n",
      "Epoch 279: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2002 - val_loss: 0.4079\n",
      "Epoch 280/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2073\n",
      "Epoch 280: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2063 - val_loss: 0.3912\n",
      "Epoch 281/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.1995\n",
      "Epoch 281: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1985 - val_loss: 0.4718\n",
      "Epoch 282/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1898\n",
      "Epoch 282: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1881 - val_loss: 0.3839\n",
      "Epoch 283/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.2190\n",
      "Epoch 283: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2176 - val_loss: 0.6178\n",
      "Epoch 284/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.1967\n",
      "Epoch 284: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1956 - val_loss: 0.3855\n",
      "Epoch 285/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2007\n",
      "Epoch 285: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1987 - val_loss: 0.4269\n",
      "Epoch 286/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1991\n",
      "Epoch 286: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2050 - val_loss: 0.4653\n",
      "Epoch 287/300\n",
      "130/137 [===========================>..] - ETA: 0s - loss: 0.2083\n",
      "Epoch 287: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.3726\n",
      "Epoch 288/300\n",
      "126/137 [==========================>...] - ETA: 0s - loss: 0.1829\n",
      "Epoch 288: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1827 - val_loss: 0.3914\n",
      "Epoch 289/300\n",
      "132/137 [===========================>..] - ETA: 0s - loss: 0.2107\n",
      "Epoch 289: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2094 - val_loss: 0.3880\n",
      "Epoch 290/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.2020\n",
      "Epoch 290: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2013 - val_loss: 0.4282\n",
      "Epoch 291/300\n",
      "131/137 [===========================>..] - ETA: 0s - loss: 0.2042\n",
      "Epoch 291: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.5458\n",
      "Epoch 292/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.1935\n",
      "Epoch 292: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1924 - val_loss: 0.4254\n",
      "Epoch 293/300\n",
      "125/137 [==========================>...] - ETA: 0s - loss: 0.2086\n",
      "Epoch 293: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2034 - val_loss: 0.4149\n",
      "Epoch 294/300\n",
      "125/137 [==========================>...] - ETA: 0s - loss: 0.1865\n",
      "Epoch 294: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2093 - val_loss: 0.3391\n",
      "Epoch 295/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2030\n",
      "Epoch 295: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2019 - val_loss: 0.3999\n",
      "Epoch 296/300\n",
      "135/137 [============================>.] - ETA: 0s - loss: 0.1834\n",
      "Epoch 296: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1832 - val_loss: 0.4292\n",
      "Epoch 297/300\n",
      "134/137 [============================>.] - ETA: 0s - loss: 0.1869\n",
      "Epoch 297: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1855 - val_loss: 0.3970\n",
      "Epoch 298/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2074\n",
      "Epoch 298: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.3943\n",
      "Epoch 299/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.2040\n",
      "Epoch 299: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.2030 - val_loss: 0.3731\n",
      "Epoch 300/300\n",
      "133/137 [============================>.] - ETA: 0s - loss: 0.1786\n",
      "Epoch 300: val_loss did not improve from 0.32258\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 0.1827 - val_loss: 0.4134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x158199249a0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=SGD(momentum=0.02), loss=CategoricalCrossentropy())\n",
    "\n",
    "model.fit(x=train_x,\n",
    "         y=train_y,\n",
    "         batch_size=128,\n",
    "         epochs=300,\n",
    "         validation_split=0.3,\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b586030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./models/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1c315d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_value(row):\n",
    "    if (row == 0):\n",
    "        return '+'\n",
    "    elif (row == 1) :\n",
    "        return '-'\n",
    "\n",
    "def remap(y):\n",
    "    res= y.argmax(axis=1)\n",
    "    df = pd.DataFrame(res, columns=['Predicted'])\n",
    "    df['Predicted'] = df['Predicted'].apply(change_value)\n",
    "    return df\n",
    "\n",
    "def predict_result(model, x):    \n",
    "    y_pred = model.predict(x)\n",
    "    return remap(y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "72707a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_result(model, train_x)\n",
    "y_true = remap(train_y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6c823be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.88728\n",
      "f1: 0.88728\n"
     ]
    }
   ],
   "source": [
    "print(f\"acc: {accuracy_score(y_pred,y_true)}\")\n",
    "print(f\"f1: {f1_score(y_pred,y_true, average='micro')}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7385cbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1581a4dd1c0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d46c26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_result(model, test_x)\n",
    "y_true = remap(test_y.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a562d1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.855\n",
      "f1: 0.855\n"
     ]
    }
   ],
   "source": [
    "print(f\"acc: {accuracy_score(y_pred,y_true)}\")\n",
    "print(f\"f1: {f1_score(y_pred,y_true, average='micro')}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5821b950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1581a4e6550>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447aed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
